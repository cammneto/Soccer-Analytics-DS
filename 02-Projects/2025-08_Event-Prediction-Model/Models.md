# Model Log

## Architectures
- LSTM sequence model
- Transformer encoder only

## Training
- Batch size = 64, Seq length = 20
- GPU mixed precision enabled
